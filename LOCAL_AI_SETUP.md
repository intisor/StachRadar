# ğŸ¤– StackRadar Local AI Setup Guide

## Prerequisites

- **Ollama** - Local LLM runtime (free, open-source)
- **.NET 8 Runtime**
- **Docker** (optional, for Ollama)

---

## ğŸš€ Installation Steps

### **1. Install Ollama**

#### **Windows**
```powershell
# Download from https://ollama.ai/download/windows
# Or use Windows Package Manager
winget install Ollama.Ollama
```

#### **macOS**
```bash
# Download from https://ollama.ai/download/mac
# Or use Homebrew
brew install ollama
```

#### **Linux**
```bash
curl https://ollama.ai/install.sh | sh
```

#### **Docker (All Platforms)**
```bash
docker run -d -p 11434:11434 ollama/ollama
```

---

### **2. Start Ollama Service**

#### **Windows**
```powershell
# Ollama runs as a service after installation
# Verify it's running:
curl http://localhost:11434/api/tags
```

#### **macOS/Linux**
```bash
ollama serve
```

---

### **3. Pull AI Models (Choose One)**

For StackRadar, we recommend lightweight models for fast extraction:

#### **Option A: Mistral (7B) - RECOMMENDED**
```bash
ollama pull mistral:latest
# Fast, accurate, optimized for extraction tasks
```

#### **Option B: Neural Chat (7B)**
```bash
ollama pull neural-chat:latest
# Good for conversation and analysis
```

#### **Option C: Dolphin Mixtral**
```bash
ollama pull dolphin-mixtral:latest
# More powerful but slower
```

#### **Option D: Llama 2 (7B)**
```bash
ollama pull llama2:latest
# General purpose, reliable
```

---

### **4. Verify Setup**

```bash
# Test Ollama is running
curl http://localhost:11434/api/tags

# Test with a simple prompt
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "mistral:latest",
  "prompt": "What is .NET?"
}'
```

---

## ğŸ“‹ Configuration

### **Update StackRadar for Local AI**

In `Program.cs` (Scout command), the LocalAiAnalyzer is configured with:

```csharp
// Default configuration
LocalAiEndpoint: "http://localhost:11434"
Model: "mistral:latest"  // Change to your preferred model
```

### **Custom Configuration**

Create `appsettings.json`:

```json
{
  "LocalAi": {
    "Endpoint": "http://localhost:11434",
    "Model": "mistral:latest",
    "Temperature": 0.3,
    "MaxTokens": 500
  }
}
```

---

## ğŸ¯ StackRadar Commands

### **Scrape .NET Job Sites**

```bash
# Basic job site scraping
dotnet run --project StackRadar.Cli -- scout --source dotnetjobs --limit 50

# With verbose logging
dotnet run --project StackRadar.Cli -- scout --source dotnetjobs --limit 100 --verbose

# Save to specific file
dotnet run --project StackRadar.Cli -- scout --source dotnetjobs --output netjobs.txt
```

### **What Happens**

1. âœ… Loads `dotnet_job_sites.csv` with 15 job sites
2. âœ… Scrapes each site for .NET job listings
3. âœ… Sends HTML to local Ollama for analysis
4. âœ… Extracts company names using AI
5. âœ… Saves discovered companies to output file

---

## ğŸ“Š Expected Output

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”¯â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”“
â”‚ Source          â”‚ Targets â”‚ Output           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dotnetjobs      â”‚ 50      â”‚ discovered.txt   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¤– AI extracted companies:
- Microsoft
- Google
- Amazon
- Facebook
- Apple
- [... more companies ...]
```

---

## ğŸ”§ Troubleshooting

### **Local AI Not Connecting**

```
Error: Cannot connect to local AI at http://localhost:11434
```

**Solution:**
```bash
# Verify Ollama is running
curl http://localhost:11434/api/tags

# If not running:
# Windows: Service should auto-start. Restart if needed.
# macOS/Linux: Run 'ollama serve' in terminal
```

### **Model Not Found**

```
Error: model "mistral:latest" not found
```

**Solution:**
```bash
# Pull the model
ollama pull mistral:latest

# List available models
ollama list
```

### **Slow Performance**

**Solution:**
- Use smaller model: `neural-chat:latest` or `phi:latest`
- Increase model RAM allocation
- Check system resources (RAM, CPU)

### **High Memory Usage**

**Solution:**
- Use smaller quantized models
- Run on GPU if available: `ollama pull mistral:q4_0`

---

## ğŸš€ Performance Tips

### **Recommended Models by Performance**

| Model | Size | Speed | Quality | RAM |
|-------|------|-------|---------|-----|
| phi:latest | 2.7B | âš¡âš¡âš¡ | â­â­â­ | 4GB |
| neural-chat | 7B | âš¡âš¡ | â­â­â­â­ | 8GB |
| mistral:latest | 7B | âš¡âš¡ | â­â­â­â­ | 8GB |
| llama2:latest | 7B | âš¡âš¡ | â­â­â­â­â­ | 8GB |

### **Optimization Tips**

1. **Use quantized models** for faster performance:
   ```bash
   ollama pull mistral:q4_0  # 4-bit quantization
   ```

2. **Batch processing** - Process multiple pages before AI analysis

3. **Caching** - Cache extraction results to avoid re-processing

---

## ğŸ“ˆ Next Steps

1. âœ… Install Ollama and pull a model
2. âœ… Run basic job site scraping
3. âœ… Analyze extracted companies
4. âœ… Cross-reference with your ASP.NET domain list
5. âœ… Perform manual LinkedIn searches for high-value leads

---

## ğŸ¤ Support Resources

- **Ollama Docs**: https://ollama.ai
- **Model Library**: https://ollama.ai/library
- **GitHub**: https://github.com/jmorganca/ollama
- **Discord Community**: https://discord.gg/ollama

---

## ğŸ’¡ Example Workflows

### **Workflow 1: Quick Company Discovery**
```bash
# Step 1: Scrape job sites for .NET companies
dotnet run --project StackRadar.Cli -- scout --source dotnetjobs --limit 100 --output companies.txt

# Step 2: Review results
type companies.txt | more

# Step 3: Manually search top companies on LinkedIn
```

### **Workflow 2: Intelligence Gathering**
```bash
# Step 1: Scrape companies
dotnet run -- scout --source dotnetjobs --limit 50 --verbose

# Step 2: Perform detailed analysis
# - Company size from job postings
# - Tech stack requirements
# - Hiring frequency
# - Growth trajectory
```

---

**Happy scraping! ğŸš€**
